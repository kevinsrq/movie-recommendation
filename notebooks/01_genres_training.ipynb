{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information retrieval for movies recommendation\n",
    "\n",
    "Database which the project it's based on:   \n",
    "[HBO Max](https://www.kaggle.com/datasets/dgoenrique/hbo-max-movies-and-tv-shows)  \n",
    "\n",
    "<div></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from toolbox import preprocessing\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load\n",
    "try: \n",
    "    pop_movies = pd.read_csv('../data/modified/popular_movies.csv.zip')['tmdbId']\n",
    "except: \n",
    "# Load the \"rating\" and \"link\" with the id from multiple sources\n",
    "    ratings = pd.read_csv('../data/origin/ratings.zip')\n",
    "    links = pd.read_csv('../data/origin/links.zip')\n",
    "\n",
    "    # Inner join between both files\n",
    "    ratings = ratings.merge(links, how='inner', on='movieId')\n",
    "\n",
    "    # Select only the movies with at least 750 reviews, to classify as popular enough for recommendation\n",
    "    pop_movies = ratings['movieId'].value_counts().to_frame().query('count > 50').index\n",
    "    pop_movies = links.query('movieId in @pop_movies')['tmdbId'].dropna()\n",
    "\n",
    "    pop_movies.to_csv('../data/modified/popular_movies.csv.zip', index=False, compression='zip')\n",
    "\n",
    "    del ratings, links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div></div> \n",
    "\n",
    "## Leitura dos Arquivos\n",
    "\n",
    "As bases vieram em formato CSV, portanto, só foi utilizado o pandas para leitura e feito um concat\n",
    "\n",
    "<div></div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kevin\\AppData\\Local\\Temp\\ipykernel_11576\\1593278502.py:5: DtypeWarning: Columns (10) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  dt_m = pd.read_csv('../data/origin/movies_metadata.zip')\n"
     ]
    }
   ],
   "source": [
    "# Load the 'credits' dataset from a zipped CSV file\n",
    "dt_c = pd.read_csv('../data/origin/credits.zip')\n",
    "\n",
    "# Load the 'movies_metadata' dataset from a zipped CSV file\n",
    "dt_m = pd.read_csv('../data/origin/movies_metadata.zip')\n",
    "\n",
    "# Convert the 'id' column to numeric data type, ignoring any errors\n",
    "dt_m['id'] = pd.to_numeric(dt_m['id'], errors='coerce')\n",
    "\n",
    "# Convert the 'popularity' column to numeric data type, ignoring any errors\n",
    "dt_m['popularity'] = pd.to_numeric(dt_m['popularity'], errors='coerce')\n",
    "\n",
    "# Merge the 'movies_metadata' DataFrame with the 'credits' DataFrame based on the 'id' column\n",
    "dt_m = dt_m.merge(dt_c.set_index('id'), how='left', left_on=['id'], right_index=True)\n",
    "\n",
    "# Drop rows with missing values in the 'id' column\n",
    "dt_m.dropna(subset=['id', 'overview'], inplace=True)\n",
    "\n",
    "# Select the movies with the minimun engagement\n",
    "dt_m.query('id in @pop_movies', inplace=True)\n",
    "\n",
    "# Reset index \n",
    "dt_m.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Delete the 'credits' DataFrame to free up memory\n",
    "del dt_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_m.drop(['homepage', 'original_title', 'poster_path'], axis=1, inplace=True, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_m['p_cast'] = dt_m['cast'].map(lambda cast: [x.get('name').lower().replace(' ', '_') for x in ast.literal_eval(cast)[0:5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_director(string): \n",
    "    crew = ast.literal_eval(string)\n",
    "\n",
    "    for i in crew: \n",
    "        if i.get('job').lower()=='director':\n",
    "            return  i.get('name').lower().replace(' ', '_')\n",
    "\n",
    "dt_m['director'] = dt_m['crew'].map(get_director)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_m['p_genres'] = dt_m['genres'].map(lambda genres: [genre.get('name').lower().replace(' ', '_') for genre in ast.literal_eval(genres)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_m['p_director'] = dt_m['director'].map(lambda dir: [dir])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_m.eval('docs = p_cast + p_genres + p_director', inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identificação das query / docs\n",
    "\n",
    "Foi feita uma separação do index das query, para pode fazer uma localização do na base origina após o TF-IDF, dado que o TF-IDF reseta os index dos termos por documento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = dt_m['docs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2374647, 3032250)"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Word2Vec(min_count=1,\n",
    "                 window=3,\n",
    "                 vector_size=30,\n",
    "                #  sample=0, \n",
    "                 alpha=0.03, \n",
    "                 negative=5,\n",
    "                 workers=mp.cpu_count()-1, \n",
    "                 seed=42, \n",
    "                 sg=1)\n",
    "\n",
    "model.build_vocab(docs)\n",
    "model.train(docs, total_examples=model.corpus_count, epochs=50, report_delay=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "remover = set(model.wv.index_to_key)\n",
    "\n",
    "dt_m['docs'] = dt_m['docs'].map(lambda doc: [x for x in doc if x in remover])\n",
    "dt_m['docs'] = dt_m['docs'].map(lambda doc: [w for w in list(doc) if w is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_m['embed_docs'] = dt_m['docs'].map(lambda doc: sum(model.wv[(w)] for w in list(doc)) / len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [[0.9999998]]\n",
       "2333    [[0.98740584]]\n",
       "6148    [[0.96926284]]\n",
       "5533     [[0.9664703]]\n",
       "1270     [[0.9592312]]\n",
       "             ...      \n",
       "4899    [[0.15361409]]\n",
       "4513    [[0.15361409]]\n",
       "719     [[0.15335658]]\n",
       "5316    [[0.13914518]]\n",
       "3609    [[0.13711804]]\n",
       "Name: embed_docs, Length: 7142, dtype: object"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity = dt_m['embed_docs'].map(lambda doc: cosine_similarity(dt_m['embed_docs'][0].reshape(1, -1), doc.reshape(1, -1))).sort_values(ascending=False)\n",
    "similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [0.03979972, -0.35979572, -0.24217868, 0.03807...\n",
       "1       [0.037879337, -0.3519118, -0.23652212, 0.03519...\n",
       "2       [0.027425002, -0.32364863, -0.20734666, 0.0291...\n",
       "3       [0.043172404, -0.33960414, -0.22528216, 0.0378...\n",
       "4       [0.05631381, -0.30987737, -0.2057415, 0.054088...\n",
       "                              ...                        \n",
       "7137    [0.05709718, -0.40053213, -0.26335394, 0.04953...\n",
       "7138    [0.056247674, -0.40096593, -0.25617376, 0.0453...\n",
       "7139    [0.056247674, -0.40096593, -0.25617376, 0.0453...\n",
       "7140    [0.038822003, -0.32833073, -0.20914164, 0.0373...\n",
       "7141    [0.038822003, -0.32833073, -0.20914164, 0.0373...\n",
       "Name: docs, Length: 7142, dtype: object"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt_m['docs'].map(lambda doc: sum(model.wv[word] for word in doc) / len(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../models/metadata_model.txt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
